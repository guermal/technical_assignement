# # ğŸ§  Technical Assignment

This repository contains the complete solution for a technical assignment involving dummy video dataset generation, vision and vision-language models, and training/inference pipelines.

---

## ğŸ“¦ Data Creation

To generate the dummy dataset, run:

```bash
python dummy_data.py
```

This creates a folder named `synthetic_dataset/` with the following structure:

```text
synthetic_dataset/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ class_0/
â”‚   â”‚   â”œâ”€â”€ video_0.pt
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ class_1/
â”‚   â””â”€â”€ ...
â””â”€â”€ test/
    â”œâ”€â”€ class_0/
    â”œâ”€â”€ class_1/
    â””â”€â”€ ...
```

Each `.pt` file is a PyTorch tensor of shape `[T, C, H, W]` representing a synthetic video.

---

## ğŸ—ï¸ Model Building

The `build_model.py` script defines three types of models:

### ğŸ”¹ Vision Transformer (ViT)

A fully vision-based transformer model (e.g., TimeSformer, VideoMAE) loaded from Hugging Face and followed by a classification head.

### ğŸ”¹ Vision-Language Models (VLMs)

Two types:
- **Classification**: Vision encoder + LLM (e.g., BERT)
- **Captioning**: Vision encoder + Causal LLM (e.g., GPT-2)

---

## ğŸ‹ï¸â€â™‚ï¸ Model Training

### â–¶ï¸ Video Captioning

```bash
python main.py \
  --vision_model timesformer \
  --use_vlm True \
  --language_model gpt2 \
  --use_text True \
  --captioning True \
  --freeze_visionbackbone True \
  --use_lora_llm True
```

ğŸ“Œ Notes:
- `--use_lora_llm` is optional (default is `False` in `config_file.yaml`)
- `--freeze_visionbackbone` is recommended for single GPU training

---

### â–¶ï¸ Video Classification (Vision Only)

```bash
python main.py --vision_model timesformer
```

Optional flags:
- `--freeze_visionbackbone True` â†’ train only the classification head
- `--use_lora_vision True` â†’ apply LoRA for lightweight fine-tuning

---

### â–¶ï¸ Video Classification (Vision + Language)

```bash
python main.py \
  --vision_model timesformer \
  --language_model bert \
  --use_vlm True \
  --freeze_visionbackbone True
```

Use `--use_text True` to combine vision and text embeddings.

ğŸ“Œ All models save checkpoints to the `./checkpoints/` directory.

---

## ğŸ” Inference

To run inference, use the same CLI args as training, but replace `main.py` with `inference.py`.

### Example: Captioning Inference

```bash
python inference.py \
  --vision_model timesformer \
  --use_vlm True \
  --language_model gpt2 \
  --use_text False \
  --captioning True \
  --freeze_visionbackbone True \
  --use_lora_llm True \
  --video_dir ./frames_see/
```

ğŸ“Œ You must specify `--video_dir` with frames for inference.  
ğŸ“Œ Checkpoints must be saved in `./checkpoints/`.

---

## ğŸ› ï¸ Frame Generation for Inference

To convert dummy test tensors into image frames for inference:

```bash
python create_dummy.py
```

This will:
- Load the test videos from `dummy_data.py`
- Convert them into frames
- Save the frames to `./frames_see/`

---

## âœ… Summary

| Task                | Script            | Description                          |
|---------------------|-------------------|--------------------------------------|
| Dataset Generation  | `dummy_data.py`   | Creates dummy video tensors          |
| Model Definitions   | `build_model.py`  | ViT, VLM, classification, captioning |
| Training            | `main.py`         | Flexible CLI with transformer options|
| Inference           | `inference.py`    | Matches CLI used in training         |
| Frame Conversion    | `create_dummy.py` | Converts tensor â†’ image sequences    |

---
